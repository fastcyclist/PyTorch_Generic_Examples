{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Generic Examples\n",
    "#### This is a practice of what's on https://machinelearningmastery.com/pytorch-tutorial-develop-deep-learning-models/\n",
    "#### Data is from http://archive.ics.uci.edu/ml/datasets/Heart+Disease\n",
    "#### & http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/\n",
    "#### (processed.cleveland.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How to Confirm PyTorch Is Installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0+cpu\n"
     ]
    }
   ],
   "source": [
    "# PyTorch version check\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyTorch Deep Learning Model Life-Cycle\n",
    "\n",
    "In this section, you will discover the life-cycle for a deep learning model and the PyTorch API that you can use to define models.\n",
    "\n",
    "A model has a life-cycle, and this very simple knowledge provides the backbone for both modeling a dataset and understanding the PyTorch API.\n",
    "\n",
    "The five steps in the life-cycle are as follows:\n",
    "\n",
    "- 1. Prepare the Data.\n",
    "- 2. Define the Model.\n",
    "- 3. Train the Model.\n",
    "- 4. Evaluate the Model.\n",
    "- 5. Make Predictions.\n",
    "\n",
    "Let’s take a closer look at each step in turn.\n",
    "\n",
    "<b>Note</b>: There are many ways to achieve each of these steps using the PyTorch API, although I have aimed to show you the simplest, or most common, or most idiomatic.\n",
    "\n",
    "If you discover a better approach, let me know in the comments below.\n",
    "\n",
    "### Step 1: Prepare the Data\n",
    "\n",
    "The first step is to load and prepare your data.\n",
    "\n",
    "Neural network models require numerical input data and numerical output data.\n",
    "\n",
    "You can use standard Python libraries to load and prepare tabular data, like CSV files. For example, Pandas can be used to load your CSV file, and tools from scikit-learn can be used to encode categorical data, such as class labels.\n",
    "\n",
    "PyTorch provides the [Dataset class](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) that you can extend and customize to load your dataset.\n",
    "\n",
    "For example, the constructor of your dataset object can load your data file (e.g. a CSV file). You can then override the <i>\\__len__\\()</i> function that can be used to get the length of the dataset (number of rows or samples), and the <i>\\__getitem__\\()</i> function that is used to get a specific sample by index.\n",
    "\n",
    "When loading your dataset, you can also perform any required transforms, such as scaling or encoding.\n",
    "\n",
    "Once loaded, PyTorch provides the [DataLoader class](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) to navigate a Dataset instance during the training and evaluation of your model.\n",
    "\n",
    "A <i>DataLoader</i> instance can be created for the training dataset, test dataset, and even a validation dataset.\n",
    "\n",
    "The [random_split()](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split) function can be used to split a dataset into train and test sets. Once split, a selection of rows from the Dataset can be provided to a DataLoader, along with the batch size and whether the data should be shuffled every epoch. Once defined, a <i>DataLoader</i> can be enumerated, yielding one batch worth of samples each iteration.\n",
    "\n",
    "### Step 2: Define the Model\n",
    "\n",
    "The next step is to define a model.\n",
    "\n",
    "The idiom for defining a model in PyTorch involves defining a class that extends the [Module class](https://pytorch.org/docs/stable/nn.html#module).\n",
    "\n",
    "The constructor of your class defines the layers of the model and the forward() function is the override that defines how to forward propagate input through the defined layers of the model.\n",
    "\n",
    "Many layers are available, such as [Linear](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear) for fully connected layers, [Conv2d](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d) for convolutional layers, and [MaxPool2d](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxPool2d) for pooling layers.\n",
    "\n",
    "Activation functions can also be defined as layers, such as [ReLU](https://pytorch.org/docs/stable/nn.html#torch.nn.ReLU), [Softmax](https://pytorch.org/docs/stable/nn.html#torch.nn.Softmax), and [Sigmoid](https://pytorch.org/docs/stable/nn.html#torch.nn.Sigmoid).\n",
    "\n",
    "The weights of a given layer can also be initialized after the layer is defined in the constructor.\n",
    "\n",
    "Common examples include the [Xavier](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_uniform_) and [He weight](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_uniform_) initialization schemes.\n",
    "\n",
    "### Step 3: Train the Model\n",
    "\n",
    "The training process requires that you define a loss function and an optimization algorithm.\n",
    "\n",
    "Common loss functions include the following:\n",
    "\n",
    "- [BCELoss](https://pytorch.org/docs/stable/nn.html#torch.nn.BCELoss): Binary cross-entropy loss for binary classification.\n",
    "- [CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss): Categorical cross-entropy loss for multi-class classification.\n",
    "- [MSELoss](https://pytorch.org/docs/stable/nn.html#torch.nn.MSELoss): Mean squared loss for regression.\n",
    "\n",
    "For more on loss functions generally, see the tutorial:\n",
    "\n",
    "- [Loss and Loss Functions for Training Deep Learning Neural Networks](https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/)\n",
    "\n",
    "Stochastic gradient descent is used for optimization, and the standard algorithm is provided by the [SGD class](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD), although other versions of the algorithm are available, such as [Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam).\n",
    "\n",
    "Training the model involves enumerating the <i>DataLoader</i> for the training dataset.\n",
    "\n",
    "First, a loop is required for the number of training epochs. Then an inner loop is required for the mini-batches for stochastic gradient descent.\n",
    "\n",
    "Each update to the model involves the same general pattern comprised of:\n",
    "\n",
    "- Clearing the last error gradient.\n",
    "- A forward pass of the input through the model.\n",
    "- Calculating the loss for the model output.\n",
    "- Backpropagating the error through the model.\n",
    "- Update the model in an effort to reduce loss.\n",
    "\n",
    "### Step 4: Evaluate the model\n",
    "Once the model is fit, it can be evaluated on the test dataset.\n",
    "\n",
    "This can be achieved by using the <i>DataLoader</i> for the test dataset and collecting the predictions for the test set, then comparing the predictions to the expected values of the test set and calculating a performance metric.\n",
    "\n",
    "### Step 5: Make predictions\n",
    "A fit model can be used to make a prediction on new data.\n",
    "\n",
    "For example, you might have a single image or a single row of data and want to make a prediction.\n",
    "\n",
    "This requires that you wrap the data in a [PyTorch Tensor](https://pytorch.org/docs/stable/tensors.html) data structure.\n",
    "\n",
    "A Tensor is just the PyTorch version of a NumPy array for holding data. It also allows you to perform the automatic differentiation tasks in the model graph, like calling backward() when training the model.\n",
    "\n",
    "The prediction too will be a Tensor, although you can retrieve the NumPy array by [detaching the Tensor](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach) from the automatic differentiation graph and calling the NumPy function.\n",
    "\n",
    "Now that we are familiar with the PyTorch API at a high-level and the model life-cycle, let’s look at how we can develop some standard deep learning models from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. How to Develop PyTorch Deep Learning Models\n",
    "\n",
    "In this section, you will discover how to develop, evaluate, and make predictions with standard deep learning models, including Multilayer Perceptrons (MLP) and Convolutional Neural Networks (CNN).\n",
    "\n",
    "A Multilayer Perceptron model, or MLP for short, is a standard fully connected neural network model.\n",
    "\n",
    "It is comprised of layers of nodes where each node is connected to all outputs from the previous layer and the output of each node is connected to all inputs for nodes in the next layer.\n",
    "\n",
    "An MLP is a model with one or more fully connected layers. This model is appropriate for tabular data, that is data as it looks in a table or spreadsheet with one column for each variable and one row for each variable. There are three predictive modeling problems you may want to explore with an MLP; they are binary classification, multiclass classification, and regression.\n",
    "\n",
    "Let’s fit a model on a real dataset for each of these cases.\n",
    "\n",
    "<b>Note</b>: The models in this section are effective, but not optimized. See if you can improve their performance. Post your findings in the comments below.\n",
    "\n",
    "### 3.1. How to Develop an MLP for Binary Classification\n",
    "\n",
    "We will use the Ionosphere binary (two class) classification dataset to demonstrate an MLP for binary classification.\n",
    "\n",
    "This dataset involves predicting whether there is a structure in the atmosphere or not given radar returns.\n",
    "\n",
    "The dataset will be downloaded automatically using Pandas, but you can learn more about it here.\n",
    "\n",
    "- [Ionosphere Dataset (csv)](https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv).\n",
    "- [Ionosphere Dataset Description](https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.names).\n",
    "\n",
    "We will use a [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) to encode the string labels to integer values 0 and 1. The model will be fit on 67 percent of the data, and the remaining 33 percent will be used for evaluation, split using the [train_test_split() function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
    "\n",
    "It is a good practice to use <i>'relu'</i> activation with a <i>'He Uniform'</i> weight initialization. This combination goes a long way to overcome the problem of [vanishing gradients](https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/) when training deep neural network models. For more on ReLU, see the tutorial:\n",
    "\n",
    "- [A Gentle Introduction to the Rectified Linear Unit (ReLU)](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/)\n",
    "\n",
    "The model predicts the probability of class 1 and uses the sigmoid activation function. The model is optimized using stochastic gradient descent and seeks to minimize the [binary cross-entropy loss](https://machinelearningmastery.com/cross-entropy-for-machine-learning/).\n",
    "\n",
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203 100\n",
      "Accuracy: 0.840\n",
      "Predicted: 0.041 (class=0)\n"
     ]
    }
   ],
   "source": [
    "# pytorch mlp for binary classification\n",
    "from numpy import vstack\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "# dataset definition\n",
    "class CSVDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self, path):\n",
    "        # load the csv file as a dataframe\n",
    "        df = read_csv(path, header=None)\n",
    "        # store the inputs and outputs\n",
    "        self.X = df.values[:, :-1]\n",
    "        self.y = df.values[:, -1]\n",
    "        # ensure input data is floats\n",
    "        self.X = self.X.astype('float32')\n",
    "        # label encode target and ensure the values are floats\n",
    "        self.y = LabelEncoder().fit_transform(self.y)\n",
    "        self.y = self.y.astype('float32')\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    "\n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    "\n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])\n",
    "\n",
    "# model definition\n",
    "class MLP(Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # input to first hidden layer *******************************\n",
    "        self.hidden1 = Linear(n_inputs, 10)    # Applies a linear transformation to the incoming data\n",
    "                                               # n_inputs features going in, 10 features going out.\n",
    "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "                                               # Fills the input Tensor with values according to the He initialization.\n",
    "                                               # nonlinearity – the non-linear function (nn.functional name), \n",
    "                                               #                recommended to use only with 'relu' or 'leaky_relu' (default).\n",
    "        self.act1 = ReLU()                     # Applies the rectified linear unit function element-wise.\n",
    "        # second hidden layer ***************************************\n",
    "        self.hidden2 = Linear(10, 8)\n",
    "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "        self.act2 = ReLU()\n",
    "        # third hidden layer and output *****************************\n",
    "        self.hidden3 = Linear(8, 1)\n",
    "        xavier_uniform_(self.hidden3.weight)\n",
    "        self.act3 = Sigmoid()\n",
    "\n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer *******************************\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "        # second hidden layer ***************************************\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # third hidden layer and output *****************************\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "        return X\n",
    "\n",
    "# prepare the dataset\n",
    "def prepare_data(path):\n",
    "    # load the dataset\n",
    "    dataset = CSVDataset(path)\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    return train_dl, test_dl\n",
    "\n",
    "# train the model\n",
    "def train_model(train_dl, model):\n",
    "    # define the optimization\n",
    "    criterion = BCELoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # enumerate epochs\n",
    "    for epoch in range(100):\n",
    "        # enumerate mini batches\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model(inputs)\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, targets)\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()\n",
    "\n",
    "# evaluate the model\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # round to class values\n",
    "        yhat = yhat.round()\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate accuracy\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    return acc\n",
    "\n",
    "# make a class prediction for one row of data\n",
    "def predict(row, model):\n",
    "    # convert row to data\n",
    "    row = Tensor([row])             # A multi-dimensional matrix containing elements of a single data type.\n",
    "    # make prediction\n",
    "    yhat = model(row)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()    # This separates the answer prediction from the gradient function 'grad_fn' info.\n",
    "    return yhat\n",
    "\n",
    "# prepare the data\n",
    "path = 'processed.cleveland.data.imputed_normalized'\n",
    "#path = 'ionosphere.csv'\n",
    "train_dl, test_dl = prepare_data(path)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))\n",
    "\n",
    "# define the network\n",
    "model = MLP(13)    # 13 predictors for 1 target.\n",
    "#model = MLP(34)    # 34 predictors for 1 target.\n",
    "\n",
    "# train the model\n",
    "train_model(train_dl, model)\n",
    "\n",
    "# evaluate the model\n",
    "acc = evaluate_model(test_dl, model)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "\n",
    "# make a single prediction (expect class=1)\n",
    "row = [0.71,1.00,0.00,0.48,0.24,1.00,1.00,0.60,0.00,0.37,1.00,0.00,0.75]\n",
    "#row = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\n",
    "yhat = predict(row, model)\n",
    "print('Predicted: %.3f (class=%d)' % (yhat, yhat.round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first reports the shape of the train and test datasets, then fits the model and evaluates it on the test dataset. Finally, a prediction is made for a single row of data.\n",
    "\n",
    "<b>Note</b>: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. How to Develop an MLP for Multiclass Classification\n",
    "\n",
    "We will use the Iris flowers multiclass classification dataset to demonstrate an MLP for multiclass classification.\n",
    "\n",
    "This problem involves predicting the species of iris flower given measures of the flower.\n",
    "\n",
    "The dataset will be downloaded automatically using Pandas, but you can learn more about it here.\n",
    "\n",
    "- [Iris Dataset (csv)](https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv)\n",
    "- [Iris Dataset Description](https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.names)\n",
    "\n",
    "Given that it is a multiclass classification, the model must have one node for each class in the output layer and use the softmax activation function. The loss function is the cross entropy, which is appropriate for integer encoded class labels (e.g. 0 for one class, 1 for the next class, etc.).\n",
    "\n",
    "The complete example of fitting and evaluating an MLP on the iris flowers dataset is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 50\n",
      "Accuracy: 0.920\n",
      "Predicted: [[9.9965000e-01 3.5002874e-04 2.6980703e-20]] (class=0)\n"
     ]
    }
   ],
   "source": [
    "# pytorch mlp for multiclass classification\n",
    "from numpy import vstack\n",
    "from numpy import argmax\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Softmax\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "# dataset definition\n",
    "class CSVDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self, path):\n",
    "        # load the csv file as a dataframe\n",
    "        df = read_csv(path, header=None)\n",
    "        # store the inputs and outputs\n",
    "        self.X = df.values[:, :-1]\n",
    "        self.y = df.values[:, -1]\n",
    "        # ensure input data is floats\n",
    "        self.X = self.X.astype('float32')\n",
    "        # label encode target and ensure the values are floats\n",
    "        self.y = LabelEncoder().fit_transform(self.y)\n",
    "\n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    "\n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])\n",
    "\n",
    "# model definition\n",
    "class MLP(Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # input to first hidden layer *******************************\n",
    "        self.hidden1 = Linear(n_inputs, 10)    # Applies a linear transformation to the incoming data\n",
    "                                               # n_inputs features going in, 10 features going out.\n",
    "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "                                               # Fills the input Tensor with values according to the He initialization.\n",
    "                                               # nonlinearity – the non-linear function (nn.functional name), \n",
    "                                               #                recommended to use only with 'relu' or 'leaky_relu' (default).\n",
    "        self.act1 = ReLU()                     # Applies the rectified linear unit function element-wise.\n",
    "        # second hidden layer ***************************************\n",
    "        self.hidden2 = Linear(10, 8)\n",
    "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "        self.act2 = ReLU()\n",
    "        # third hidden layer and output *****************************\n",
    "        self.hidden3 = Linear(8, 3)            # Here the output layer has 3 choices since we're trying to choose among 3.\n",
    "        xavier_uniform_(self.hidden3.weight)\n",
    "        self.act3 = Softmax(dim=1)\n",
    "\n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer *******************************\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "        # second hidden layer ***************************************\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # output layer **********************************************\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "        return X\n",
    "\n",
    "# prepare the dataset\n",
    "def prepare_data(path):\n",
    "    # load the dataset\n",
    "    dataset = CSVDataset(path)\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    return train_dl, test_dl\n",
    "\n",
    "# train the model\n",
    "def train_model(train_dl, model):\n",
    "    # define the optimization\n",
    "    criterion = CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # enumerate epochs\n",
    "    for epoch in range(500):\n",
    "        # enumerate mini batches\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model(inputs)\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, targets)\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()\n",
    "\n",
    "# evaluate the model\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        # convert to class labels\n",
    "        yhat = argmax(yhat, axis=1)\n",
    "        # reshape for stacking\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        yhat = yhat.reshape((len(yhat), 1))\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate accuracy\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    return acc\n",
    "\n",
    "# make a class prediction for one row of data\n",
    "def predict(row, model):\n",
    "    # convert row to data\n",
    "    row = Tensor([row])             # A multi-dimensional matrix containing elements of a single data type.\n",
    "    # make prediction\n",
    "    yhat = model(row)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()    # This separates the answer prediction from the gradient function 'grad_fn' info.\n",
    "    return yhat\n",
    "\n",
    "# prepare the data\n",
    "path = 'iris.csv'\n",
    "train_dl, test_dl = prepare_data(path)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))\n",
    "\n",
    "# define the network\n",
    "model = MLP(4)\n",
    "\n",
    "# train the model\n",
    "train_model(train_dl, model)\n",
    "\n",
    "# evaluate the model\n",
    "acc = evaluate_model(test_dl, model)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "\n",
    "# make a single prediction\n",
    "row = [5.1,3.5,1.4,0.2]\n",
    "yhat = predict(row, model)\n",
    "print('Predicted: %s (class=%d)' % (yhat, argmax(yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first reports the shape of the train and test datasets, then fits the model and evaluates it on the test dataset. Finally, a prediction is made for a single row of data.\n",
    "\n",
    "<b>Note</b>: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. How to Develop an MLP for Regression\n",
    "\n",
    "We will use the Boston housing regression dataset to demonstrate an MLP for regression predictive modeling.\n",
    "\n",
    "This problem involves predicting house value based on properties of the house and neighborhood.\n",
    "\n",
    "The dataset will be downloaded automatically using Pandas, but you can learn more about it here.\n",
    "\n",
    "- [Boston Housing Dataset (csv)](https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv)\n",
    "- [Boston Housing Dataset Description](https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.names)\n",
    "\n",
    "This is a regression problem that involves predicting a single numeric value. As such, the output layer has a single node and uses the default or linear activation function (no activation function). The mean squared error (mse) loss is minimized when fitting the model.\n",
    "\n",
    "Recall that this is regression, not classification; therefore, we cannot calculate classification accuracy. For more on this, see the tutorial:\n",
    "\n",
    "- [Difference Between Classification and Regression in Machine Learning](https://machinelearningmastery.com/classification-versus-regression-in-machine-learning/)\n",
    "\n",
    "The complete example of fitting and evaluating an MLP on the Boston housing dataset is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
